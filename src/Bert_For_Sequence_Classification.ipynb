{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Bert is Used in Text Classification \n",
    "We will use the pre-trained BERT transformer model and then fine-tune it on propoganda detection. The main benefit of using a Transformer model is that the learning is bi-directional.\n",
    "\n",
    "<b>HuggingFace:</b> One of the best libraries for implementing transformers in python. You can install the library with *pip install transformers*. You will also need pytorch installed (pip install torch)\n",
    "\n",
    "#### Pre Processing the Data \n",
    "To use a pre-trained BERT model the input data must be converted into the required input data format. From there the input data is sent to BERT to obtain the embeddings. Bert embeddings are trained with a classification task and a next sentence prediction task (to determine if a sentence naturally follows the previous one). \n",
    "\n",
    "<b>Classification Task</b> - requires single vector input representing the input sentence. Token [CLS] is chosen. This is what we will be using for our propaganda classification. \n",
    "\n",
    "<b>Next Sentence Prediction Task</b> - where we inform the model where the first sentence ends and the next begins. [SEP] is usd. If each input sample contains only one text input (for classifier) we add [SEP] at the end of the input text. \n",
    "\n",
    "<b>Padding Token</b>: \n",
    "BERT requires a fixed length of text per input. When we have sentences that are shorter than the maximum length, we have to add paddings (empty toknes) to the end of the sentence. Represented by [PAD]\n",
    "\n",
    "<b>Converting Tokens to IDs</b>: \n",
    "BERT has given a unique ID to each token. We need to convert each token in the input sentence into its corresponding unique BERT ID. \n",
    "\n",
    "<b>Note on Vocabulary</b>: Since BERT is a pre-trained model that was trained on a particular corpus, the vocabulary is fixed. Therefore it is possible that some of the words in our data are OOV. We should replace these okens with a special token [UNK] which stansd for the unknown token. BERT also makes use a of WordPiece algoirithm that breaks a word into several subwords. This reduces the number of [UNK] tokens. Without applying the tokenization function of BERT before converting tokens to ids, OOV words will not be split into subwords. \n",
    "\n",
    "<b>Key Steps:</b>\n",
    "1. Applying tokenizer: BertTokenizer.from_pretrained(\"bert-base-cased\").tokenize \n",
    "2. Adding the [CLS] token at the beginning of the sentence \n",
    "3. Adding the [SEP] token at the end of the sentence \n",
    "4. Padding he setence with [PAD] tokens so that the total length equals to the maximum leength \n",
    "4. Converting each token to its corresponding BERT ID using t.convert_tokens_to_ids(tokens)\n",
    "\n",
    "<b>Using Transformers Package:</b>\n",
    "The function encode_plus does all of the above steps at once. Ie. adds the [CLS] & [SEP], sets max length, adds [PADS] and generates an attention mask & IDs in tensor format. The attention mask tells the model which of the tokens are [PAD] tokens and should not be added. Needed for BERT.\n",
    "\n",
    "Q - what is different about tensor format?\n",
    "\n",
    "<b>Distilled BERT:</b>\n",
    "Smaller & Faster version of BERT that retains 95% of BERT performances while only using 60% of the bert-based-uncased parameters. \n",
    "\n",
    "<b>Hyperparameters:</b> \n",
    "1. Model Type - bert-based-cased\n",
    "2. Max number of tokens - 60 \n",
    "3. Batch size - 64 \n",
    "4. Optimizer - AdamW \n",
    "5. Learning Rate - 5e-5\n",
    "6. Epochs - 5 \n",
    "7. Loss criterion - nn.BCELoss(), used \n",
    "8. Gradient Norm Clipper - yes (used to prevent to exploding gradient problem)\n",
    "\n",
    "<b> Conclusions: </b> \n",
    "1. Using the loss criterion lowers the validation loss but increases the training loss - brings the values closer together\n",
    "2. It is a sign of overfitting when the validation starts to increase while the training loss continues to decrease. In this case we should use the model with the lowest validation loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/Brigit/anaconda3/lib/python3.7/site-packages (4.0.0)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: filelock in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.10)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from transformers) (4.31.1)\n",
      "Requirement already satisfied: numpy in /Users/Brigit/.local/lib/python3.7/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: packaging in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from transformers) (19.0)\n",
      "Requirement already satisfied: sacremoses in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: requests in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: six in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (2.3.1)\n",
      "Requirement already satisfied: joblib in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/Brigit/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in propoganda data \n",
    "Dataset = pd.read_csv('cleaned_propaganda_data.csv', sep = '|', header = None, names = [\"Text\", \"Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split Data into Training, Test & Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the Dataset into train (64%) , validation (16%) and test sets(20%)\n",
    "def split_datasets(dataset):\n",
    "    trainval, test_data, trainval_labels, test_labels = train_test_split(dataset['Text'], dataset['Label'], \n",
    "                                                                    random_state=0, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    stratify=dataset['Label'])\n",
    "    train_data, validation_data, train_labels, val_labels = train_test_split(trainval, trainval_labels, \n",
    "                                                                random_state=0, \n",
    "                                                                test_size=0.2, \n",
    "                                                                stratify=trainval_labels)\n",
    "    return train_data,validation_data,test_data,train_labels,validation_labels,test_labels\n",
    "\n",
    "train_data,validation_data,test_data,train_labels,validation_labels,test_labels= split_datasets(Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Tokenize each Dataset to use as Input for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode each input of text as a unique BERT ID and include CLS, SEP & PAD tokens \n",
    "def bert_tokenizer(train_data, validation_data, test_data):\n",
    "    train_tokens = tokenizer.batch_encode_plus(\n",
    "        train_data.tolist(),  # the sentence to be encoded\n",
    "        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "        truncation=True, #truncates sentences longer than max length \n",
    "        max_length = 60,  # maximum length of a sentence -> need to decide what we want this to be \n",
    "        padding=True,  # Add [PAD]s\n",
    "        return_attention_mask = True,  # Generate the attention mask\n",
    "        return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    #Validation set encoding \n",
    "    validation_tokens = tokenizer.batch_encode_plus(\n",
    "        validation_data.tolist(),  # the sentence to be encoded\n",
    "        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "        truncation=True, #truncates sentences longer than max length \n",
    "        max_length = 60,  # maximum length of a sentence -> need to decide what we want this to be \n",
    "        padding=True,  # Add [PAD]s\n",
    "        return_attention_mask = True,  # Generate the attention mask\n",
    "        return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    #Test set encoding\n",
    "    test_tokens = tokenizer.batch_encode_plus(\n",
    "        test_data.tolist(),  # the sentence to be encoded\n",
    "        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "        truncation=True, #truncates sentences longer than max length \n",
    "        max_length = 60,  # maximum length of a sentence -> need to decide what we want this to be \n",
    "        padding=True,  # Add [PAD]s\n",
    "        return_attention_mask = True,  # Generate the attention mask\n",
    "        return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    return train_tokens, validation_tokens, test_tokens\n",
    "\n",
    "\n",
    "def create_tensor_datasets(train_tokens, train_labels, validation_tokens, validation_labels, test_tokens, test_labels):\n",
    "    with torch.no_grad():\n",
    "        #Get input_ids, attention_mask, token_type_ids for each dataset & convert labels to tensors\n",
    "        train_ids = train_tokens['input_ids']\n",
    "        train_mask = train_tokens['attention_mask']\n",
    "        train_tt_ids = train_tokens['token_type_ids']\n",
    "        train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "        val_ids = validation_tokens['input_ids']\n",
    "        val_mask = validation_tokens['attention_mask']\n",
    "        val_tt_ids = validation_tokens['token_type_ids']\n",
    "        val_y = torch.tensor(validation_labels.tolist())\n",
    "\n",
    "        test_ids = test_tokens['input_ids']\n",
    "        test_mask = test_tokens['attention_mask']\n",
    "        test_tt_ids = test_tokens['token_type_ids']\n",
    "        test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "    #Create a tensor dataset for each of the train,test & validation sets \n",
    "    train_tensor_dataset = TensorDataset(train_ids, train_mask, train_tt_ids, train_y) #wrap all tensor data for training\n",
    "    validation_tensor_dataset = TensorDataset(val_ids, val_mask, val_tt_ids, val_y)\n",
    "    test_tensor_dataset = TensorDataset(test_ids, test_mask, test_tt_ids, test_y)\n",
    "    \n",
    "    return train_tensor_dataset, validation_tensor_dataset, test_tensor_dataset\n",
    "\n",
    "#Function to create the DataLoaders that load the data in batches \n",
    "def data_loader(train_data, validation_data,test_data, batch_size): \n",
    "    \n",
    "    train_sample = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sample, batch_size=batch_size)\n",
    "    \n",
    "    validation_sample = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sample, batch_size=batch_size)\n",
    "    \n",
    "    test_sample = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sample, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, validation_dataloader, test_dataloader\n",
    "\n",
    "#Step 1: Define the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "#Step 2: Tokenize each dataset \n",
    "train_tokens, validation_tokens, test_tokens = bert_tokenizer(train_data, validation_data, test_data)\n",
    "\n",
    "#Step 3: Create tensor datasets\n",
    "train_tensor_dataset, validation_tensor_dataset, test_tensor_dataset = create_tensor_datasets(train_tokens, train_labels, validation_tokens, validation_labels, test_tokens, test_labels)\n",
    "\n",
    "#Step 4: Create dataloaders for each tensor dataset\n",
    "batch_size = 64\n",
    "train_dataloader, validation_dataloader, test_dataloader = data_loader(train_tensor_dataset,validation_tensor_dataset, test_tensor_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build Bert For Sequence Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Define Model \n",
    "def define_model(learning_rate): \n",
    "    bertmodel = BertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = bertmodel.to(device) \n",
    "\n",
    "    #Define Optimizer - using AdamW \n",
    "    optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "    return model, optimizer, device\n",
    "learning_rate = 5e-5\n",
    "model, optimizer,device = define_model(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Training the Model\n",
    "\n",
    "Initially: Define number of epoachs & steps per validation & put model into training mode\n",
    "\n",
    "For each epoch: \n",
    "1. Enumerate through the training dataset dataloader (each batch of data)  \n",
    "2. For each batch: \n",
    "  - push batch to GPU to get the datafields (ids, mask, token_type_ids, labels) \n",
    "  - perform a forward pass of the model to get the output \n",
    "  - get the loss from the ouptut \n",
    "  - remove previous calculated gradients (optimizer.zero_grad())\n",
    "  - perform backwards pass of model to calculate new gradients \n",
    "  - clip gradients to prevent the exploding gradient problem (potential to remove step)\n",
    "  - update parameters in optimizer \n",
    "  - add loss to the running loss \n",
    "3. At the end of each epoch evaluate the current model on the validation set \n",
    "  - will return the validation accuracy & loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Bert(model, optimizer, train_loader, val_loader): \n",
    "    \n",
    "    num_epochs = 5\n",
    "    \n",
    "    #initialize values \n",
    "    running_training_loss = 0.0\n",
    "    total_steps = 0\n",
    "    training_loss_list = []\n",
    "    validation_loss_list = []\n",
    "    validation_accuracy_list = []\n",
    "    total_steps_list = []\n",
    "    models_built = []\n",
    "    #criterion = nn.BCELoss();\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs): #for each epoch\n",
    "        model.train() #put model in training mode \n",
    "\n",
    "        for step, batch in enumerate(train_loader): # iterate over batches\n",
    "        \n",
    "            if step % 50 == 0 and not step == 0:# progress update after every 50 batches.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_loader)))\n",
    "\n",
    "            data_ids, mask, tt_ids, labels  = [r.to(device) for r in batch] # push the batch to gpu      \n",
    "\n",
    "            #perform foward pass \n",
    "            output = model(input_ids= data_ids, attention_mask= mask,token_type_ids = tt_ids, labels = labels) # get model predictions for the current batch\n",
    "            loss = output.loss\n",
    "            \n",
    "            optimizer.zero_grad() #remove any previously calculated gradients \n",
    "            loss.backward() # perform backward pass to calculate the gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "\n",
    "            optimizer.step() # update parameters\n",
    "            \n",
    "            # update running values\n",
    "            running_training_loss += loss.item()\n",
    "            total_steps += 1\n",
    "\n",
    "        #Calculate average training loss over the entire dataset for this epoach\n",
    "        average_training_loss = running_training_loss /len(train_loader)\n",
    "        running_training_loss = 0.0\n",
    "\n",
    "        #Measure the performance of the model on the validation dataset after each training epoch \n",
    "        validation_loss, validation_accuracy = evaluate_val(model, val_loader)\n",
    "\n",
    "        training_loss_list.append(average_training_loss)\n",
    "        validation_loss_list.append(validation_loss)\n",
    "        validation_accuracy_list.append(validation_accuracy)\n",
    "        total_steps_list.append(total_steps)\n",
    "        models_built.append(model)\n",
    "\n",
    "        print('Epoch [{}/{}], Training Loss: {:.4f}, Valid Loss: {:.4f}, Validation Accuracy: {:.4f}'\n",
    "                      .format(epoch+1,num_epochs, average_training_loss, validation_loss, validation_accuracy))\n",
    "\n",
    "    print('Finished Training!')\n",
    "    model_metrics = {'training_loss_values': training_loss_list, 'validation_loss_values': validation_loss_list,\n",
    "                     'validation_accuracy_values': validation_accuracy_list, 'total_steps_values': total_steps_list,\n",
    "                     'models_built': models_built}\n",
    "    return model, model_metrics\n",
    "\n",
    "def evaluate_val(model, val_loader): \n",
    "    model.eval() #put the model into evaluation mode (disables dropout layers)\n",
    "    print(\"evaluation\")\n",
    "\n",
    "    #Accuracy & Loss Variables (will be updated in batches)\n",
    "    validation_running_loss = 0.0\n",
    "    predicted_values = []\n",
    "    true_values = []\n",
    "\n",
    "    with torch.no_grad():                   \n",
    "        for step, batch in enumerate(val_loader):# validation loop\n",
    "            data_ids, mask, tt_ids, labels  = [r.to(device) for r in batch]\n",
    "            output = model(input_ids= data_ids, attention_mask= mask,token_type_ids = tt_ids, labels = labels) # get model predictions for the current batch\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "            predicted_values.extend(torch.argmax(logits ,dim=1).tolist())\n",
    "            true_values.extend(labels.tolist())\n",
    "            \n",
    "            #Update running loss\n",
    "            validation_running_loss += loss.item()\n",
    "    \n",
    "    #Calculate total loss & accuracy \n",
    "    validation_accuracy = accuracy_score(true_values, predicted_values)\n",
    "    validation_loss = validation_running_loss/len(val_loader)\n",
    "\n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "model, model_metrics = train_Bert(model, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph of validation loss vs. train loss \n",
    "def get_loss_graph(model_metrics):\n",
    "    training_loss = model_metrics.get('training_loss_values')\n",
    "    validation_loss = model_metrics.get('validation_loss_values')\n",
    "    validation_accuracy = model_metrics.get('validation_accuracy_values')\n",
    "    total_training_steps = model_metrics.get('total_steps_values')\n",
    "\n",
    "    plt.plot(total_training_steps, training_loss, label='Training')\n",
    "    plt.plot(total_training_steps, validation_loss, label='Validation')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "\n",
    "# Evaluation of Model - Classification report & confusion matrix \n",
    "def evaluate_model(model, test_loader):\n",
    "    predicted_values = []\n",
    "    true_values = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(test_loader): # iterate over batches\n",
    "\n",
    "            data_ids, mask, tt_ids, labels  = [r.to(device) for r in batch] # push the batch to gpu      \n",
    "\n",
    "            output = model(input_ids= data_ids, attention_mask= mask,token_type_ids = tt_ids, labels = labels) # get model predictions for the current batch\n",
    "            logits = output.logits\n",
    "            predicted_values.extend(torch.argmax(logits,dim=1).tolist())\n",
    "            true_values.extend(labels.tolist())\n",
    "\n",
    "            \n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(true_values, predicted_values, labels=[1,0], digits=4))\n",
    "    \n",
    "    cm = confusion_matrix(true_values, predicted_values, labels=[1,0])\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "\n",
    "    ax.xaxis.set_ticklabels(['PROPAGANDA', 'REAL NEWS'])\n",
    "    ax.yaxis.set_ticklabels(['PROPAGANDA', 'REAL NEWS'])\n",
    "\n",
    "#Step 1: Get graph of training loss vs. validation loss \n",
    "get_loss_graph(model_metrics)\n",
    "\n",
    "#Step 2: Get model results in from of classification report & confusion matrix \n",
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
